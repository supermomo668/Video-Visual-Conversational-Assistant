{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "# First, we use SQLAlchemy to setup a simple sqlite db:\n",
    "\n",
    "from sqlalchemy import create_engine, MetaData, Table, Column, String, Integer, select, column\n",
    "\n",
    "engine = create_engine(\"sqlite:///:memory:\")\n",
    "metadata_obj = MetaData(bind=engine)\n",
    "\n",
    "# We then create a toy city_stats table:\n",
    "\n",
    "# create city SQL table\n",
    "table_name = \"city_stats\"\n",
    "city_stats_table = Table(\n",
    "    table_name,\n",
    "    metadata_obj,\n",
    "    Column(\"city_name\", String(16), primary_key=True),\n",
    "    Column(\"population\", Integer),\n",
    "    Column(\"country\", String(16), nullable=False),\n",
    ")\n",
    "metadata_obj.create_all()\n",
    "# Now it’s time to insert some datapoints!\n",
    "\n",
    "# If you want to look into filling into this table by inferring structured datapoints from unstructured data, take a look at the below section. Otherwise, you can choose to directly populate this table:\n",
    "\n",
    "from sqlalchemy import insert\n",
    "rows = [\n",
    "    {\"city_name\": \"Toronto\", \"population\": 2731571, \"country\": \"Canada\"},\n",
    "    {\"city_name\": \"Tokyo\", \"population\": 13929286, \"country\": \"Japan\"},\n",
    "    {\"city_name\": \"Berlin\", \"population\": 600000, \"country\": \"Germany\"},\n",
    "]\n",
    "for row in rows:\n",
    "    stmt = insert(city_stats_table).values(**row)\n",
    "    with engine.connect() as connection:\n",
    "        cursor = connection.execute(stmt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring Structured Datapoints\n",
    "LlamaIndex offers the capability to convert unstructured datapoints to structured data. In this section, we show how we can populate the city_stats table by ingesting Wikipedia articles about each city.\n",
    "\n",
    "First, we use the Wikipedia reader from LlamaHub to load some pages regarding the relevant data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we use the Wikipedia reader from LlamaHub to load some pages regarding the relevant data.\n",
    "\n",
    "from llama_index import download_loader\n",
    "\n",
    "WikipediaReader = download_loader(\"WikipediaReader\")\n",
    "wiki_docs = WikipediaReader().load_data(pages=['Toronto', 'Berlin', 'Tokyo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <!-- When we build the SQL index, we can specify these docs as the first input; these documents will be converted to structured datapoints and inserted into the db: -->\n",
    "\n",
    "from llama_index import GPTSQLStructStoreIndex, SQLDatabase\n",
    "\n",
    "sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n",
    "# NOTE: the table_name specified here is the table that you\n",
    "# want to extract into from unstructured documents.\n",
    "index = GPTSQLStructStoreIndex.from_documents(\n",
    "    wiki_docs, \n",
    "    sql_database=sql_database, \n",
    "    table_name=\"city_stats\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we can wrap the SQLAlchemy engine with our SQLDatabase wrapper; this allows the db to be used within LlamaIndex:\n",
    "\n",
    "from llama_index import SQLDatabase\n",
    "\n",
    "sql_database = SQLDatabase(engine, include_tables=[\"city_stats\"])\n",
    "\n",
    "# If the db is already populated with data, we can instantiate the SQL index with a blank documents list. Otherwise see the below section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can take a look at the current table to verify that the datapoints have been inserted!\n",
    "\n",
    "# view current table\n",
    "stmt = select(\n",
    "    [column(\"city_name\"), column(\"population\"), column(\"country\")]\n",
    ").select_from(city_stats_table)\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    results = connection.execute(stmt).fetchall()\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-to-SQL (basic)\n",
    "LlamaIndex offers “text-to-SQL” capabilities, both at a very basic level and also at a more advanced level. In this section, we show how to make use of these text-to-SQL capabilities at a basic level.\n",
    "\n",
    "A simple example is shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Logging to DEBUG for more detailed outputs\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(\"Which city has the highest population?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the underlying derived SQL query through response.extra_info['sql_query']. It should look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "SELECT city_name, population\n",
    "FROM city_stats\n",
    "ORDER BY population DESC\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Injecting Context\n",
    "By default, the text-to-SQL prompt just injects the table schema information into the prompt. However, oftentimes you may want to add your own context as well. This section shows you how you can add context, either manually, or extracted through documents.\n",
    "\n",
    "We offer you a context builder class to better manage the context within your SQL tables: SQLContextContainerBuilder. This class takes in the SQLDatabase object, and a few other optional parameters, and builds a SQLContextContainer object that you can then pass to the index during construction + query-time.\n",
    "\n",
    "You can add context manually to the context builder. The code snippet below shows you how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.struct_store import SQLContextContainerBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually set text\n",
    "city_stats_text = (\n",
    "    \"This table gives information regarding the population and country of a given city.\\n\"\n",
    "    \"The user will query with codewords, where 'foo' corresponds to population and 'bar'\"\n",
    "    \"corresponds to city.\"\n",
    ")\n",
    "table_context_dict={\"city_stats\": city_stats_text}\n",
    "context_builder = SQLContextContainerBuilder(sql_database, context_dict=table_context_dict)\n",
    "context_container = context_builder.build_context_container()\n",
    "\n",
    "# building the index\n",
    "index = GPTSQLStructStoreIndex.from_documents(\n",
    "    wiki_docs, \n",
    "    sql_database=sql_database, \n",
    "    table_name=\"city_stats\",\n",
    "    sql_context_container=context_container\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also choose to extract context from a set of unstructured Documents. To do this, you can call SQLContextContainerBuilder.from_documents. We use the TableContextPrompt and the RefineTableContextPrompt (see the reference docs).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this is a dummy document that we will extract context from\n",
    "# in GPTSQLContextContainerBuilder\n",
    "city_stats_text = (\n",
    "    \"This table gives information regarding the population and country of a given city.\\n\"\n",
    ")\n",
    "context_documents_dict = {\"city_stats\": [Document(city_stats_text)]}\n",
    "context_builder = SQLContextContainerBuilder.from_documents(\n",
    "    context_documents_dict, \n",
    "    sql_database\n",
    ")\n",
    "context_container = context_builder.build_context_container()\n",
    "\n",
    "# building the index\n",
    "index = GPTSQLStructStoreIndex.from_documents(\n",
    "    wiki_docs, \n",
    "    sql_database=sql_database, \n",
    "    table_name=\"city_stats\",\n",
    "    sql_context_container=context_container,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Table Context within an Index\n",
    "A database collection can have many tables, and if each table has many columns + a description associated with it, then the total context can be quite large.\n",
    "\n",
    "Luckily, you can choose to use a LlamaIndex data structure to store this table context! Then when the SQL index is queried, we can use this “side” index to retrieve the proper context that can be fed into the text-to-SQL prompt.\n",
    "\n",
    "Here we make use of the derive_index_from_context function within SQLContextContainerBuilder to create a new index. You have flexibility in choosing which index class to specify + which arguments to pass in. We then use a helper method called query_index_for_context which is a simple wrapper on the query call that wraps a query template + stores the context on the generated context container.\n",
    "\n",
    "You can then build the context container, and pass it to the index during query-time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import GPTSQLStructStoreIndex, SQLDatabase, GPTVectorStoreIndex\n",
    "from llama_index.indices.struct_store import SQLContextContainerBuilder\n",
    "\n",
    "sql_database = SQLDatabase(engine)\n",
    "# build a vector index from the table schema information\n",
    "context_builder = SQLContextContainerBuilder(sql_database)\n",
    "table_schema_index = context_builder.derive_index_from_context(\n",
    "    GPTVectorStoreIndex,\n",
    "    store_index=True\n",
    ")\n",
    "\n",
    "query_str = \"Which city has the highest population?\"\n",
    "\n",
    "# query the table schema index using the helper method\n",
    "# to retrieve table context\n",
    "SQLContextContainerBuilder.query_index_for_context(\n",
    "    table_schema_index,\n",
    "    query_str,\n",
    "    store_context_str=True\n",
    ")\n",
    "\n",
    "# query the SQL index with the table context\n",
    "query_engine = index.as_query_engine()\n",
    "response = query_engine.query(query_str, sql_context_container=context_container)\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision-cap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
